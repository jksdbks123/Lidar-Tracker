{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from DDBSCAN import Raster_DBSCAN\n",
    "from Models import *\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import matplotlib.colors as mcolors\n",
    "# times new roman font\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "seed = 414\n",
    "# np.random.seed(seed)\n",
    "colors = np.random.rand(600, 3)\n",
    "colors = np.concatenate([np.array([[0,0,0]]),colors],axis = 0)\n",
    "colormap = mcolors.ListedColormap(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneSocialLSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size=1,             # Position coordinate (x only)\n",
    "                 output_size=2,            # Parameters for output distribution (mean, std)\n",
    "                 embedding_size=64,        # Embedding dimension\n",
    "                 rnn_size=128,             # LSTM hidden state size\n",
    "                 grid_size=32,             # Number of lane cells to consider for social context\n",
    "                 dropout=0.2,              # Dropout probability\n",
    "                 use_cuda=True,            # GPU acceleration\n",
    "                 seq_length=12,            # Sequence length for training\n",
    "                 gru=False,                # Use GRU instead of LSTM\n",
    "                 infer=False):             # Inference mode\n",
    "        \"\"\"\n",
    "        Lane-based Social LSTM implementation adapted from Alahi et al. 2016\n",
    "        Modified to work with 1D lane cell representation\n",
    "        \"\"\"\n",
    "        super(LaneSocialLSTM, self).__init__()\n",
    "\n",
    "        # Store parameters\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_size = rnn_size\n",
    "        self.grid_size = grid_size  # Number of lane cells for social context\n",
    "        self.use_cuda = use_cuda and torch.cuda.is_available()\n",
    "        self.infer = infer\n",
    "        self.gru = gru\n",
    "        \n",
    "        # Sequence length depends on training or inference\n",
    "        self.seq_length = 1 if infer else seq_length\n",
    "\n",
    "        # Linear embedding layers\n",
    "        self.input_embedding_layer = nn.Linear(self.input_size, self.embedding_size)\n",
    "        self.tensor_embedding_layer = nn.Linear(self.grid_size*self.rnn_size, self.embedding_size)\n",
    "        \n",
    "        # RNN Cell (LSTM or GRU)\n",
    "        if self.gru:\n",
    "            self.cell = nn.GRUCell(2*self.embedding_size, self.rnn_size)\n",
    "        else:\n",
    "            self.cell = nn.LSTMCell(2*self.embedding_size, self.rnn_size)\n",
    "            \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(self.rnn_size, self.output_size)\n",
    "        \n",
    "        # Activation and regularization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_social_tensor(self, lane_occupancy, hidden_states):\n",
    "        \"\"\"\n",
    "        Computes the social tensor using lane occupancy states\n",
    "        \n",
    "        Args:\n",
    "            lane_occupancy: Lane occupancy masks [num_vehicles, grid_size]\n",
    "                            1 if a cell is occupied by another vehicle, 0 otherwise\n",
    "            hidden_states: Hidden states of all vehicles [num_vehicles, rnn_size]\n",
    "            \n",
    "        Returns:\n",
    "            social_tensor: Pooled tensor of lane information\n",
    "        \"\"\"\n",
    "        num_vehicles = lane_occupancy.size(0)\n",
    "        \n",
    "        # Create empty tensor to hold social context\n",
    "        social_tensor = torch.zeros(num_vehicles, self.grid_size, self.rnn_size)\n",
    "        if self.use_cuda:\n",
    "            social_tensor = social_tensor.cuda()\n",
    "        \n",
    "        # For each vehicle, compute its social context along the lane\n",
    "        for vehicle in range(num_vehicles):\n",
    "            # Multiply lane occupancy mask with hidden states to get pooled states\n",
    "            # For each cell in the lane, add the hidden states of vehicles present there\n",
    "            for cell in range(self.grid_size):\n",
    "                # Find which vehicles occupy this cell (excluding the current vehicle)\n",
    "                for other_vehicle in range(num_vehicles):\n",
    "                    if other_vehicle != vehicle and lane_occupancy[other_vehicle, cell] > 0:\n",
    "                        # Add the hidden state of the occupying vehicle\n",
    "                        social_tensor[vehicle, cell] += hidden_states[other_vehicle] * lane_occupancy[other_vehicle, cell]\n",
    "        \n",
    "        # Reshape to form the final social tensor\n",
    "        social_tensor = social_tensor.view(num_vehicles, self.grid_size*self.rnn_size)\n",
    "        return social_tensor\n",
    "    \n",
    "    def forward(self, input_data, lane_occupancy, hidden_states, cell_states, vehicle_ids, num_vehicles_per_frame, look_up):\n",
    "        \"\"\"\n",
    "        Forward pass for the Lane-based Social LSTM model\n",
    "        \n",
    "        Args:\n",
    "            input_data: Input positions [seq_length, max_num_vehicles, input_size]\n",
    "                        Contains x-coordinate in lane cells\n",
    "            lane_occupancy: Lane occupancy masks [seq_length, max_num_vehicles, grid_size]\n",
    "                           Represents lane cell occupancy around each vehicle\n",
    "            hidden_states: Hidden states [num_total_vehicles, rnn_size]\n",
    "            cell_states: Cell states [num_total_vehicles, rnn_size] \n",
    "            vehicle_ids: List of vehicle IDs present in each frame\n",
    "            num_vehicles_per_frame: Number of vehicles in each frame\n",
    "            look_up: Mapping from vehicle ID to index in hidden_states\n",
    "            \n",
    "        Returns:\n",
    "            outputs: Predicted distributions [seq_length, num_vehicles, output_size]\n",
    "            hidden_states: Updated hidden states\n",
    "            cell_states: Updated cell states\n",
    "        \"\"\"\n",
    "        # Get dimensions\n",
    "        num_total_vehicles = len(look_up)\n",
    "        \n",
    "        # Prepare output tensor\n",
    "        outputs = torch.zeros(self.seq_length * num_total_vehicles, self.output_size)\n",
    "        if self.use_cuda:\n",
    "            outputs = outputs.cuda()\n",
    "        \n",
    "        # Process each frame in the sequence\n",
    "        for frame_idx, frame in enumerate(input_data):\n",
    "            # Get IDs of vehicles present in current frame\n",
    "            current_vehicle_ids = [int(veh_id) for veh_id in vehicle_ids[frame_idx]]\n",
    "            \n",
    "            # Skip if no vehicles in this frame\n",
    "            if len(current_vehicle_ids) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get indices in the hidden/cell states for the current vehicles\n",
    "            current_indices = [look_up[x] for x in current_vehicle_ids]\n",
    "            current_indices_tensor = torch.LongTensor(current_indices)\n",
    "            if self.use_cuda:\n",
    "                current_indices_tensor = current_indices_tensor.cuda()\n",
    "            \n",
    "            # Get current frame inputs for these vehicles\n",
    "            current_input = frame[current_indices]\n",
    "            \n",
    "            # Get current lane occupancy for social pooling\n",
    "            current_lane_occupancy = lane_occupancy[frame_idx]\n",
    "            \n",
    "            # Get current hidden states\n",
    "            current_hidden = hidden_states[current_indices_tensor]\n",
    "            \n",
    "            if not self.gru:\n",
    "                current_cell = cell_states[current_indices_tensor]\n",
    "            \n",
    "            # Compute social tensor through lane-based pooling\n",
    "            social_tensor = self.create_social_tensor(current_lane_occupancy, current_hidden)\n",
    "            \n",
    "            # Embed inputs\n",
    "            input_embedded = self.dropout(self.relu(self.input_embedding_layer(current_input)))\n",
    "            \n",
    "            # Embed social tensor\n",
    "            social_embedded = self.dropout(self.relu(self.tensor_embedding_layer(social_tensor)))\n",
    "            \n",
    "            # Concatenate both embeddings\n",
    "            concat_embedded = torch.cat((input_embedded, social_embedded), dim=1)\n",
    "            \n",
    "            # Process through RNN cell\n",
    "            if self.gru:\n",
    "                next_hidden = self.cell(concat_embedded, current_hidden)\n",
    "                next_cell = None\n",
    "            else:\n",
    "                next_hidden, next_cell = self.cell(concat_embedded, (current_hidden, current_cell))\n",
    "            \n",
    "            # Compute outputs\n",
    "            frame_outputs = self.output_layer(next_hidden)\n",
    "            \n",
    "            # Insert into the right positions in the output tensor\n",
    "            for i, idx in enumerate(current_indices):\n",
    "                outputs[frame_idx * num_total_vehicles + idx] = frame_outputs[i]\n",
    "            \n",
    "            # Update hidden and cell states\n",
    "            hidden_states[current_indices_tensor] = next_hidden\n",
    "            if not self.gru:\n",
    "                cell_states[current_indices_tensor] = next_cell\n",
    "        \n",
    "        # Reshape outputs to [seq_length, num_vehicles, output_size]\n",
    "        outputs_reshaped = torch.zeros(self.seq_length, num_total_vehicles, self.output_size)\n",
    "        if self.use_cuda:\n",
    "            outputs_reshaped = outputs_reshaped.cuda()\n",
    "            \n",
    "        for frame_idx in range(self.seq_length):\n",
    "            for veh_idx in range(num_total_vehicles):\n",
    "                outputs_reshaped[frame_idx, veh_idx] = outputs[frame_idx * num_total_vehicles + veh_idx]\n",
    "        \n",
    "        return outputs_reshaped, hidden_states, cell_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "val_dataset = TrajDataset(r\"D:\\TimeSpaceDiagramDataset\\EncoderDecoder_EvenlySampled_FreeflowAug_0914_5res_lanechange_signal\\100_frame\\val\",time_span)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "# find out the number of 1 over the total number of elements\n",
    "positive_ratio = []\n",
    "for batch in tqdm(val_loader):\n",
    "    post_occ_X = batch['post_occ_X'].to(device)\n",
    "    target = batch['target'].to(device)\n",
    "    speed = batch['speed_target'].to(device)\n",
    "    positive_ratio.append(np.float32(((target == 1).sum() / target.numel()).cpu().numpy())) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
