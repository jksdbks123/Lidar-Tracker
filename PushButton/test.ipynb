{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Dataset import *\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to parse video and extract frames around a specific timestamp\n",
    "def extract_video_clip(video_path, target_timestamp, video_start_time,time_window=2):\n",
    "    # Open the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video: {video_path}\")\n",
    "        return None\n",
    "    # Get video frame rate and total frame count\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calculate the frame range for the +- time_window\n",
    "    target_time = target_timestamp\n",
    "    start_time = target_time - timedelta(seconds=time_window)\n",
    "    end_time = target_time + timedelta(seconds=time_window)\n",
    "    if start_time < video_start_time:\n",
    "        start_time = video_start_time\n",
    "    if end_time > video_start_time + timedelta(seconds=total_frames / fps):\n",
    "        end_time = video_start_time + timedelta(seconds=total_frames / fps)\n",
    "    # print(f\"Target Time: {target_time}, Start Time: {start_time}, End Time: {end_time}\")\n",
    "    # Extract frames within the time range\n",
    "    frames = []\n",
    "    frame_index = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Calculate the timestamp for the current frame\n",
    "        frame_time = video_start_time + timedelta(seconds=frame_index / fps)\n",
    "        if start_time <= frame_time <= end_time:\n",
    "            frames.append(frame)\n",
    "        frame_index += 1\n",
    "        if frame_time > end_time:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "# Load Excel file\n",
    "def load_excel_data(file_path):\n",
    "    activation_df = pd.read_excel(file_path, sheet_name=\"Activation\")\n",
    "    non_activation_df = pd.read_excel(file_path, sheet_name=\"NonActivation\")\n",
    "    return activation_df, non_activation_df\n",
    "\n",
    "# Function to process dataset and extract video clips\n",
    "def process_dataset(video_dir, excel_file, output_dir, clip_save_dir, time_window=3):\n",
    "    # Load the Excel data\n",
    "    activation_df, non_activation_df = load_excel_data(excel_file)\n",
    "\n",
    "    # Create directories for saving data\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train_dir = os.path.join(output_dir, \"train\")\n",
    "    val_dir = os.path.join(output_dir, \"val\")\n",
    "    test_dir = os.path.join(output_dir, \"test\")\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    os.makedirs(clip_save_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare data for activation and non-activation\n",
    "    # all_data = []\n",
    "    for df, label in [(activation_df, 1), (non_activation_df, 0)]:\n",
    "        for _, row in tqdm(df.iterrows()):\n",
    "            clip_timestamp = row[\"timestamp\"]\n",
    "            location = row[\"location\"]\n",
    "            # convert to datetime object\n",
    "            clip_timestamp = datetime.strptime(clip_timestamp, \"%Y%m%d_%H%M%S\")\n",
    "            # Match the video file with the timestamp\n",
    "            for video_file in os.listdir(video_dir):\n",
    "                video_start_timestamp = video_file[22:].split(\".\")[0]\n",
    "                # convert to datetime object\n",
    "                video_start_timestamp = datetime.strptime(video_start_timestamp, \"%Y%m%d_%H%M%S\")\n",
    "                video_end_timestamp = video_start_timestamp + timedelta(seconds=60 * 5)\n",
    "                if video_start_timestamp <= clip_timestamp <= video_end_timestamp:\n",
    "                    video_path = os.path.join(video_dir, video_file)\n",
    "                    clip_save_path = os.path.join(clip_save_dir, f\"{label}_{row['timestamp']}_{location}.pt\")\n",
    "                    if os.path.exists(clip_save_path):\n",
    "                        continue\n",
    "                    # Extract video clip\n",
    "                    frames = extract_video_clip(video_path, clip_timestamp, video_start_timestamp,time_window)\n",
    "                    if frames is not None:\n",
    "                        data = (frames, label, row[\"timestamp\"], location)\n",
    "                        # save the clip to the clip_save_dir\n",
    "                        # clip_save_path = os.path.join(clip_save_dir, f\"{label}_{row['timestamp']}_{location}.pt\")\n",
    "                        torch.save(torch.tensor(frames), clip_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [27:08, 13.57s/it]\n",
      "402it [1:31:02, 13.59s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process the dataset\n",
    "video_dir = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\IntesectionVideo\"\n",
    "excel_file = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\Activation.xlsx\"\n",
    "output_dir = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\Dataset\"\n",
    "clip_save_dir = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\Clips\"\n",
    "time_window = 3 # seconds\n",
    "# Load the Excel data\n",
    "activation_df, non_activation_df = load_excel_data(excel_file)\n",
    "\n",
    "# Create directories for saving data\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "train_dir = os.path.join(output_dir, \"train\")\n",
    "val_dir = os.path.join(output_dir, \"val\")\n",
    "test_dir = os.path.join(output_dir, \"test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(clip_save_dir, exist_ok=True)\n",
    "\n",
    "# Prepare data for activation and non-activation\n",
    "# all_data = []\n",
    "for df, label in [(activation_df, 1), (non_activation_df, 0)]:\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        clip_timestamp = row[\"timestamp\"]\n",
    "        location = row[\"location\"]\n",
    "        # convert to datetime object\n",
    "        clip_timestamp = datetime.strptime(clip_timestamp, \"%Y%m%d_%H%M%S\")\n",
    "        # Match the video file with the timestamp\n",
    "        for video_file in os.listdir(video_dir):\n",
    "            video_start_timestamp = video_file[22:].split(\".\")[0]\n",
    "            # convert to datetime object\n",
    "            video_start_timestamp = datetime.strptime(video_start_timestamp, \"%Y%m%d_%H%M%S\")\n",
    "            video_end_timestamp = video_start_timestamp + timedelta(seconds=60 * 5)\n",
    "            if video_start_timestamp <= clip_timestamp <= video_end_timestamp:\n",
    "                video_path = os.path.join(video_dir, video_file)\n",
    "                clip_save_path = os.path.join(clip_save_dir, f\"{label}_{row['timestamp']}_{location}.pt\")\n",
    "                if os.path.exists(clip_save_path):\n",
    "                    continue\n",
    "                # Extract video clip\n",
    "                frames = extract_video_clip(video_path, clip_timestamp, video_start_timestamp,time_window)\n",
    "                if frames is not None:\n",
    "                    data = (frames, label, row[\"timestamp\"], location)\n",
    "                    # save the clip to the clip_save_dir\n",
    "                    clip_save_path = os.path.join(clip_save_dir, f\"{label}_{row['timestamp']}_{location}.mp4\")\n",
    "                    out = cv2.VideoWriter(clip_save_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (frames.shape[2], frames.shape[1]))\n",
    "                    for frame in frames:\n",
    "                        out.write(frame)\n",
    "                    out.release()\n",
    "# process_dataset(video_dir, excel_file, output_dir,clip_save_dir, time_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation with 80% and 20% respectively\n",
    "clip_save_dir = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\Clips\"\n",
    "clips = os.listdir(clip_save_dir)\n",
    "train_clips, val_clips = train_test_split(clips, test_size=0.2, random_state=714)\n",
    "train_folder = r'D:\\LiDAR_Data\\2ndPHB\\Video\\Dataset\\train'\n",
    "val_folder = r'D:\\LiDAR_Data\\2ndPHB\\Video\\Dataset\\val'\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(val_folder, exist_ok=True)\n",
    "# Move the clips to the respective folders\n",
    "for clip in train_clips:\n",
    "    src = os.path.join(clip_save_dir, clip)\n",
    "    dst = os.path.join(train_folder, clip)\n",
    "    os.rename(src, dst)\n",
    "for clip in val_clips:\n",
    "    src = os.path.join(clip_save_dir, clip)\n",
    "    dst = os.path.join(val_folder, clip)\n",
    "    os.rename(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(train_dir, val_dir, batch_size=8, transform=None):\n",
    "    train_dataset = VideoDataset(train_dir, transform=transform)\n",
    "    val_dataset = VideoDataset(val_dir, transform=transform)\n",
    "    # test_dataset = VideoDataset(test_dir, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import *\n",
    "from Dataset import *\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = r'D:\\LiDAR_Data\\2ndPHB\\Video\\Dataset\\train'\n",
    "val_folder = r'D:\\LiDAR_Data\\2ndPHB\\Video\\Dataset\\val'\n",
    "train_loader, val_loader = create_data_loaders(train_folder, val_folder, batch_size=4, transform=custom_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhChe\\anaconda3\\envs\\CV\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zhChe\\anaconda3\\envs\\CV\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to C:\\Users\\zhChe/.cache\\torch\\hub\\checkpoints\\efficientnet_b3_rwightman-b3899882.pth\n",
      "100%|██████████| 47.2M/47.2M [00:00<00:00, 109MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/104 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs=50\n",
    "learning_rate=0.0001\n",
    "run_dir = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\run\"\n",
    "model = EfficientNetLSTM().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "training_curves = {\"train\": [], \"val\": []}\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    with tqdm(total=len(train_loader), desc=\"Training\") as pbar:\n",
    "        for inputs, labels, _ in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = torch.flatten(model(inputs))\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            training_curves[\"train\"].append(loss.item())\n",
    "            pbar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "            pbar.update(1)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(val_loader), desc=\"Validation\") as pbar:\n",
    "            for inputs, labels, _ in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "                outputs = torch.flatten(model(inputs))\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                training_curves[\"val\"].append(loss.item())\n",
    "                pbar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "                pbar.update(1)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1} Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    checkpoint_path = os.path.join(run_dir, f\"epoch_{epoch + 1}.pth\")\n",
    "    torch.save(model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 27/27 [01:09<00:00,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76  8]\n",
      " [ 6 15]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate the confusion matrix\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(val_loader), desc=\"Validation\") as pbar:\n",
    "        for inputs, labels, _ in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "            outputs = torch.flatten(model(inputs))\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(outputs.cpu().numpy())\n",
    "            pbar.update(1)\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[0, 1])\n",
    "recall = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6521739130434783, 0.7142857142857143)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
