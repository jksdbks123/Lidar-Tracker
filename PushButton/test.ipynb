{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Dataset import *\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to parse video and extract frames around a specific timestamp\n",
    "def extract_video_clip(video_path, target_timestamp, video_start_time,time_window=2):\n",
    "    # Open the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video: {video_path}\")\n",
    "        return None\n",
    "    # Get video frame rate and total frame count\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calculate the frame range for the +- time_window\n",
    "    target_time = target_timestamp\n",
    "    start_time = target_time - timedelta(seconds=time_window)\n",
    "    end_time = target_time + timedelta(seconds=time_window)\n",
    "    if start_time < video_start_time:\n",
    "        start_time = video_start_time\n",
    "    if end_time > video_start_time + timedelta(seconds=total_frames / fps):\n",
    "        end_time = video_start_time + timedelta(seconds=total_frames / fps)\n",
    "    # print(f\"Target Time: {target_time}, Start Time: {start_time}, End Time: {end_time}\")\n",
    "    # Extract frames within the time range\n",
    "    frames = []\n",
    "    frame_index = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Calculate the timestamp for the current frame\n",
    "        frame_time = video_start_time + timedelta(seconds=frame_index / fps)\n",
    "        if start_time <= frame_time <= end_time:\n",
    "            frames.append(frame)\n",
    "        frame_index += 1\n",
    "        if frame_time > end_time:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "# Load Excel file\n",
    "def load_excel_data(file_path):\n",
    "    activation_df = pd.read_excel(file_path, sheet_name=\"Activation\")\n",
    "    non_activation_df = pd.read_excel(file_path, sheet_name=\"NonActivation\")\n",
    "    return activation_df, non_activation_df\n",
    "\n",
    "# Function to process dataset and extract video clips\n",
    "def process_dataset(video_dir, excel_file, output_dir, clip_save_dir, time_window=3):\n",
    "    # Load the Excel data\n",
    "    activation_df, non_activation_df = load_excel_data(excel_file)\n",
    "\n",
    "    # Create directories for saving data\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train_dir = os.path.join(output_dir, \"train\")\n",
    "    val_dir = os.path.join(output_dir, \"val\")\n",
    "    test_dir = os.path.join(output_dir, \"test\")\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    os.makedirs(clip_save_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare data for activation and non-activation\n",
    "    # all_data = []\n",
    "    for df, label in [(activation_df, 1), (non_activation_df, 0)]:\n",
    "        for _, row in tqdm(df.iterrows()):\n",
    "            clip_timestamp = row[\"timestamp\"]\n",
    "            location = row[\"location\"]\n",
    "            # convert to datetime object\n",
    "            clip_timestamp = datetime.strptime(clip_timestamp, \"%Y%m%d_%H%M%S\")\n",
    "            # Match the video file with the timestamp\n",
    "            for video_file in os.listdir(video_dir):\n",
    "                video_start_timestamp = video_file[22:].split(\".\")[0]\n",
    "                # convert to datetime object\n",
    "                video_start_timestamp = datetime.strptime(video_start_timestamp, \"%Y%m%d_%H%M%S\")\n",
    "                video_end_timestamp = video_start_timestamp + timedelta(seconds=60 * 5)\n",
    "                if video_start_timestamp <= clip_timestamp <= video_end_timestamp:\n",
    "                    video_path = os.path.join(video_dir, video_file)\n",
    "                    clip_save_path = os.path.join(clip_save_dir, f\"{label}_{row['timestamp']}_{location}.pt\")\n",
    "                    if os.path.exists(clip_save_path):\n",
    "                        continue\n",
    "                    # Extract video clip\n",
    "                    frames = extract_video_clip(video_path, clip_timestamp, video_start_timestamp,time_window)\n",
    "                    if frames is not None:\n",
    "                        data = (frames, label, row[\"timestamp\"], location)\n",
    "                        # save the clip to the clip_save_dir\n",
    "                        # clip_save_path = os.path.join(clip_save_dir, f\"{label}_{row['timestamp']}_{location}.pt\")\n",
    "                        torch.save(torch.tensor(frames), clip_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [27:08, 13.57s/it]\n",
      "402it [1:31:02, 13.59s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process the dataset\n",
    "video_dir = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\IntesectionVideo\"\n",
    "excel_file = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\Activation.xlsx\"\n",
    "output_dir = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\Dataset\"\n",
    "clip_save_dir = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\Clips\"\n",
    "time_window = 3 # seconds\n",
    "# Load the Excel data\n",
    "activation_df, non_activation_df = load_excel_data(excel_file)\n",
    "\n",
    "# Create directories for saving data\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "train_dir = os.path.join(output_dir, \"train\")\n",
    "val_dir = os.path.join(output_dir, \"val\")\n",
    "test_dir = os.path.join(output_dir, \"test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(clip_save_dir, exist_ok=True)\n",
    "\n",
    "# Prepare data for activation and non-activation\n",
    "# all_data = []\n",
    "for df, label in [(activation_df, 1), (non_activation_df, 0)]:\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        clip_timestamp = row[\"timestamp\"]\n",
    "        location = row[\"location\"]\n",
    "        # convert to datetime object\n",
    "        clip_timestamp = datetime.strptime(clip_timestamp, \"%Y%m%d_%H%M%S\")\n",
    "        # Match the video file with the timestamp\n",
    "        for video_file in os.listdir(video_dir):\n",
    "            video_start_timestamp = video_file[22:].split(\".\")[0]\n",
    "            # convert to datetime object\n",
    "            video_start_timestamp = datetime.strptime(video_start_timestamp, \"%Y%m%d_%H%M%S\")\n",
    "            video_end_timestamp = video_start_timestamp + timedelta(seconds=60 * 5)\n",
    "            if video_start_timestamp <= clip_timestamp <= video_end_timestamp:\n",
    "                video_path = os.path.join(video_dir, video_file)\n",
    "                clip_save_path = os.path.join(clip_save_dir, f\"{label}_{row['timestamp']}_{location}.pt\")\n",
    "                if os.path.exists(clip_save_path):\n",
    "                    continue\n",
    "                # Extract video clip\n",
    "                frames = extract_video_clip(video_path, clip_timestamp, video_start_timestamp,time_window)\n",
    "                if frames is not None:\n",
    "                    data = (frames, label, row[\"timestamp\"], location)\n",
    "                    # save the clip to the clip_save_dir\n",
    "                    clip_save_path = os.path.join(clip_save_dir, f\"{label}_{row['timestamp']}_{location}.mp4\")\n",
    "                    out = cv2.VideoWriter(clip_save_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (frames.shape[2], frames.shape[1]))\n",
    "                    for frame in frames:\n",
    "                        out.write(frame)\n",
    "                    out.release()\n",
    "# process_dataset(video_dir, excel_file, output_dir,clip_save_dir, time_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation with 80% and 20% respectively\n",
    "clip_save_dir = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\Clips\"\n",
    "clips = os.listdir(clip_save_dir)\n",
    "train_clips, val_clips = train_test_split(clips, test_size=0.2, random_state=714)\n",
    "train_folder = r'D:\\LiDAR_Data\\2ndPHB\\Video\\Dataset\\train'\n",
    "val_folder = r'D:\\LiDAR_Data\\2ndPHB\\Video\\Dataset\\val'\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(val_folder, exist_ok=True)\n",
    "# Move the clips to the respective folders\n",
    "for clip in train_clips:\n",
    "    src = os.path.join(clip_save_dir, clip)\n",
    "    dst = os.path.join(train_folder, clip)\n",
    "    os.rename(src, dst)\n",
    "for clip in val_clips:\n",
    "    src = os.path.join(clip_save_dir, clip)\n",
    "    dst = os.path.join(val_folder, clip)\n",
    "    os.rename(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(train_dir, val_dir, batch_size=8, transform=None):\n",
    "    train_dataset = VideoDataset(train_dir, transform=transform)\n",
    "    val_dataset = VideoDataset(val_dir, transform=transform)\n",
    "    # test_dataset = VideoDataset(test_dir, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import *\n",
    "from Dataset import *\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = r'D:\\LiDAR_Data\\2ndPHB\\Video\\Dataset\\train'\n",
    "val_folder = r'D:\\LiDAR_Data\\2ndPHB\\Video\\Dataset\\val'\n",
    "train_loader, val_loader = create_data_loaders(train_folder, val_folder, batch_size=4, transform=custom_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 61, 3, 224, 224]) tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "for i, (frames, labels, locations) in enumerate(train_loader):\n",
    "    print(frames.shape, labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "learning_rate=0.001\n",
    "run_dir = \"D:\\LiDAR_Data\\2ndPHB\\Video\\run\"\n",
    "model = ResNetLSTM().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss / len(train_loader)}, Val Loss: {val_loss / len(val_loader)}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    checkpoint_path = os.path.join(run_dir, f\"epoch_{epoch + 1}.pth\")\n",
    "    torch.save(model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet18(weights = models.ResNet18_Weights.DEFAULT)\n",
    "feature_extractor = nn.Sequential(*list(resnet.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, c, h, w = frames.shape\n",
    "x = frames.view(batch_size * seq_len, c, h, w).permute(0, 3, 1, 2)\n",
    "features = feature_extractor(x)\n",
    "features = features.view(batch_size, seq_len, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_hidden_dim = 128\n",
    "lstm_layers = 2\n",
    "input_size = 512\n",
    "lstm = nn.LSTM(input_size=input_size, hidden_size=lstm_hidden_dim, num_layers=lstm_layers, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out, _ = lstm(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 61, 128])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out = lstm_out[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
