{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Dataset import *\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to parse video and extract frames around a specific timestamp\n",
    "def extract_video_clip(video_path, target_timestamp, video_start_time,time_window=2):\n",
    "    # Open the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video: {video_path}\")\n",
    "        return None\n",
    "    # Get video frame rate and total frame count\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calculate the frame range for the +- time_window\n",
    "    target_time = target_timestamp\n",
    "    start_time = target_time - timedelta(seconds=time_window)\n",
    "    end_time = target_time + timedelta(seconds=time_window)\n",
    "    if start_time < video_start_time:\n",
    "        start_time = video_start_time\n",
    "    if end_time > video_start_time + timedelta(seconds=total_frames / fps):\n",
    "        end_time = video_start_time + timedelta(seconds=total_frames / fps)\n",
    "    # print(f\"Target Time: {target_time}, Start Time: {start_time}, End Time: {end_time}\")\n",
    "    # Extract frames within the time range\n",
    "    frames = []\n",
    "    frame_index = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Calculate the timestamp for the current frame\n",
    "        frame_time = video_start_time + timedelta(seconds=frame_index / fps)\n",
    "        if start_time <= frame_time <= end_time:\n",
    "            frames.append(frame)\n",
    "        frame_index += 1\n",
    "        if frame_time > end_time:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "# Load Excel file\n",
    "def load_excel_data(file_path):\n",
    "    activation_df = pd.read_excel(file_path, sheet_name=\"Activation\")\n",
    "    non_activation_df = pd.read_excel(file_path, sheet_name=\"NonActivation\")\n",
    "    return activation_df, non_activation_df\n",
    "\n",
    "# Function to process dataset and extract video clips\n",
    "def process_dataset(video_dir, excel_file, output_dir, clip_save_dir, time_window=3):\n",
    "    # Load the Excel data\n",
    "    activation_df, non_activation_df = load_excel_data(excel_file)\n",
    "\n",
    "    # Create directories for saving data\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train_dir = os.path.join(output_dir, \"train\")\n",
    "    val_dir = os.path.join(output_dir, \"val\")\n",
    "    test_dir = os.path.join(output_dir, \"test\")\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    os.makedirs(clip_save_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare data for activation and non-activation\n",
    "    # all_data = []\n",
    "    for df, label in [(activation_df, 1), (non_activation_df, 0)]:\n",
    "        for _, row in tqdm(df.iterrows()):\n",
    "            clip_timestamp = row[\"timestamp\"]\n",
    "            location = row[\"location\"]\n",
    "            # convert to datetime object\n",
    "            clip_timestamp = datetime.strptime(clip_timestamp, \"%Y%m%d_%H%M%S\")\n",
    "            # Match the video file with the timestamp\n",
    "            for video_file in os.listdir(video_dir):\n",
    "                video_start_timestamp = video_file[22:].split(\".\")[0]\n",
    "                # convert to datetime object\n",
    "                video_start_timestamp = datetime.strptime(video_start_timestamp, \"%Y%m%d_%H%M%S\")\n",
    "                video_end_timestamp = video_start_timestamp + timedelta(seconds=60 * 5)\n",
    "                if video_start_timestamp <= clip_timestamp <= video_end_timestamp:\n",
    "                    video_path = os.path.join(video_dir, video_file)\n",
    "                    clip_save_path = os.path.join(clip_save_dir, f\"{label}_{row['timestamp']}_{location}.pt\")\n",
    "                    if os.path.exists(clip_save_path):\n",
    "                        continue\n",
    "                    # Extract video clip\n",
    "                    frames = extract_video_clip(video_path, clip_timestamp, video_start_timestamp,time_window)\n",
    "                    if frames is not None:\n",
    "                        data = (frames, label, row[\"timestamp\"], location)\n",
    "                        # save the clip to the clip_save_dir\n",
    "                        # clip_save_path = os.path.join(clip_save_dir, f\"{label}_{row['timestamp']}_{location}.pt\")\n",
    "                        torch.save(torch.tensor(frames), clip_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [27:08, 13.57s/it]\n",
      "402it [1:31:02, 13.59s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process the dataset\n",
    "video_dir = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\IntesectionVideo\"\n",
    "excel_file = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\Activation.xlsx\"\n",
    "output_dir = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\Dataset\"\n",
    "clip_save_dir = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\Clips\"\n",
    "time_window = 3 # seconds\n",
    "# Load the Excel data\n",
    "activation_df, non_activation_df = load_excel_data(excel_file)\n",
    "\n",
    "# Create directories for saving data\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "train_dir = os.path.join(output_dir, \"train\")\n",
    "val_dir = os.path.join(output_dir, \"val\")\n",
    "test_dir = os.path.join(output_dir, \"test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(clip_save_dir, exist_ok=True)\n",
    "\n",
    "# Prepare data for activation and non-activation\n",
    "# all_data = []\n",
    "for df, label in [(activation_df, 1), (non_activation_df, 0)]:\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        clip_timestamp = row[\"timestamp\"]\n",
    "        location = row[\"location\"]\n",
    "        # convert to datetime object\n",
    "        clip_timestamp = datetime.strptime(clip_timestamp, \"%Y%m%d_%H%M%S\")\n",
    "        # Match the video file with the timestamp\n",
    "        for video_file in os.listdir(video_dir):\n",
    "            video_start_timestamp = video_file[22:].split(\".\")[0]\n",
    "            # convert to datetime object\n",
    "            video_start_timestamp = datetime.strptime(video_start_timestamp, \"%Y%m%d_%H%M%S\")\n",
    "            video_end_timestamp = video_start_timestamp + timedelta(seconds=60 * 5)\n",
    "            if video_start_timestamp <= clip_timestamp <= video_end_timestamp:\n",
    "                video_path = os.path.join(video_dir, video_file)\n",
    "                clip_save_path = os.path.join(clip_save_dir, f\"{label}_{row['timestamp']}_{location}.pt\")\n",
    "                if os.path.exists(clip_save_path):\n",
    "                    continue\n",
    "                # Extract video clip\n",
    "                frames = extract_video_clip(video_path, clip_timestamp, video_start_timestamp,time_window)\n",
    "                if frames is not None:\n",
    "                    data = (frames, label, row[\"timestamp\"], location)\n",
    "                    # save the clip to the clip_save_dir\n",
    "                    clip_save_path = os.path.join(clip_save_dir, f\"{label}_{row['timestamp']}_{location}.mp4\")\n",
    "                    out = cv2.VideoWriter(clip_save_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (frames.shape[2], frames.shape[1]))\n",
    "                    for frame in frames:\n",
    "                        out.write(frame)\n",
    "                    out.release()\n",
    "# process_dataset(video_dir, excel_file, output_dir,clip_save_dir, time_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation with 80% and 20% respectively\n",
    "clip_save_dir = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\Clips\"\n",
    "clips = os.listdir(clip_save_dir)\n",
    "train_clips, val_clips = train_test_split(clips, test_size=0.2, random_state=714)\n",
    "train_folder = r'D:\\LiDAR_Data\\2ndPHB\\Video\\Dataset\\train'\n",
    "val_folder = r'D:\\LiDAR_Data\\2ndPHB\\Video\\Dataset\\val'\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(val_folder, exist_ok=True)\n",
    "# Move the clips to the respective folders\n",
    "for clip in train_clips:\n",
    "    src = os.path.join(clip_save_dir, clip)\n",
    "    dst = os.path.join(train_folder, clip)\n",
    "    os.rename(src, dst)\n",
    "for clip in val_clips:\n",
    "    src = os.path.join(clip_save_dir, clip)\n",
    "    dst = os.path.join(val_folder, clip)\n",
    "    os.rename(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import ResNetLSTM\n",
    "from Dataset import VideoDataset,custom_transform,create_data_loaders\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from Loss import *\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(train_dir, val_dir, batch_size=8, transform=None, augmentation_dict=None):\n",
    "    train_dataset = VideoDataset(train_dir, transform=transform, augmentation_dict=augmentation_dict)\n",
    "    val_dataset = VideoDataset(val_dir, transform=transform)\n",
    "    # test_dataset = VideoDataset(test_dir, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "augmentation_dict = {\"brightness\": 0.5, \"contrast\": 0.5, \"saturation\": 0.5, \"hue\": 0.5,'h_flip':0.5, 'noise': 0.2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = r'D:\\LiDAR_Data\\2ndPHB\\Video\\Dataset\\train'\n",
    "val_folder = r'D:\\LiDAR_Data\\2ndPHB\\Video\\Dataset\\val'\n",
    "train_loader, val_loader = create_data_loaders(train_folder, val_folder, batch_size=2, transform=custom_transform, augmentation_dict=augmentation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=50\n",
    "learning_rate=0.0001\n",
    "run_dir = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\run\"\n",
    "model = ResNetLSTM().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = FocalLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "training_curves = {\"train\": [], \"val\": []}\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "for inputs, labels, _ in train_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=50\n",
    "learning_rate=0.001\n",
    "run_dir = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\run\"\n",
    "model = ResNetLSTM().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = FocalLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "training_curves = {\"train\": [], \"val\": []}\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    with tqdm(total=len(train_loader), desc=\"Training\") as pbar:\n",
    "        for inputs, labels, _ in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = torch.flatten(model(inputs))\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            training_curves[\"train\"].append(loss.item())\n",
    "            pbar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "            pbar.update(1)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(val_loader), desc=\"Validation\") as pbar:\n",
    "            for inputs, labels, _ in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "                outputs = torch.flatten(model(inputs))\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                training_curves[\"val\"].append(loss.item())\n",
    "                pbar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "                pbar.update(1)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1} Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    checkpoint_path = os.path.join(run_dir, f\"epoch_{epoch + 1}.pth\")\n",
    "    torch.save(model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 53/53 [01:18<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[82  2]\n",
      " [14  7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "model_path = r\"D:\\LiDAR_Data\\2ndPHB\\Video\\run\\model\\best_model.pth\"\n",
    "model = ResNetLSTM().to(device)\n",
    "model.load_state_dict(torch.load(model_path,weights_only=True))\n",
    "# calculate the confusion matrix\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(val_loader), desc=\"Validation\") as pbar:\n",
    "        for inputs, labels, _ in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "            outputs = torch.flatten(model(inputs))\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(outputs.cpu().numpy())\n",
    "            pbar.update(1)\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[0, 1])\n",
    "recall = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn.functional as F\n",
    "def random_brightness_adjustment(frames, brightness_factor_range=(0.5, 1.5)):\n",
    "    adjusted_frames = []\n",
    "    brightness_factor = random.uniform(*brightness_factor_range)\n",
    "    for frame in frames:\n",
    "        frame = F.adjust_brightness(F.to_pil_image(frame.astype(np.uint8)), brightness_factor)\n",
    "        adjusted_frames.append(np.array(frame))\n",
    "    return np.stack(adjusted_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomBrightnessAdjustment:\n",
    "    def __init__(self, brightness_range=(0.5, 1.5)):\n",
    "        self.brightness_range = brightness_range\n",
    "\n",
    "    def __call__(self, frames):\n",
    "        adjusted_frames = []\n",
    "        brightness_factor = random.uniform(*self.brightness_range)\n",
    "        for frame in frames:\n",
    "            frame = F.adjust_brightness(frame, brightness_factor)\n",
    "            adjusted_frames.append(frame)\n",
    "        return torch.stack(adjusted_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_brightness = RandomBrightnessAdjustment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import adjust_brightness,hflip,adjust_contrast,adjust_saturation,adjust_hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m noise_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# adjusted_img = adjust_contrast(inputs[0,0], np.random.uniform(1-contrast_factor, 1+contrast_factor))\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m adjusted_img \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(inputs[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m*\u001b[39m noise_factor\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# adjusted_img = inputs[0,0]\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# adjusted_img = hflip(inputs[0,0])\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# adjusted_img = adjust_hue(inputs[0,0],0.2)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# adjusted_img = inputs[0,0] + torch.randn_like(inputs[0,0]) * 0.2\u001b[39;00m\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(adjusted_img\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "brightness_factor = 0.5\n",
    "# adjusted_img = adjust_brightness(inputs[0,0], np.random.uniform(1-brightness_factor, 1+brightness_factor))\n",
    "contrast_factor = 0.5\n",
    "noise_factor = 0.2\n",
    "# adjusted_img = adjust_contrast(inputs[0,0], np.random.uniform(1-contrast_factor, 1+contrast_factor))\n",
    "adjusted_img = inputs[0,0] + torch.randn_like(inputs[0,0]) * noise_factor\n",
    "# adjusted_img = inputs[0,0]\n",
    "# adjusted_img = hflip(inputs[0,0])\n",
    "# adjusted_img = adjust_hue(inputs[0,0],0.2)\n",
    "# adjusted_img = inputs[0,0] + torch.randn_like(inputs[0,0]) * 0.2\n",
    "plt.imshow(adjusted_img.permute(1,2,0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7777777777777778, 0.3333333333333333)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6521739130434783, 0.7142857142857143)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
